{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5226 Project - Stage 3 - Retrospective Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jacob Truong - 34553312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contract Options Selected:\n",
    "1. All agents of opposite type (location) (4)\n",
    "2. Central clock (1)\n",
    "3. Off-the-job training (1)\n",
    "\n",
    "Total cost: 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading pickle files for *interactive visualisation*:**\n",
    "I have uploaded the pickle files for each seed used to train and evaluate the design. It is highly recommended to load the pickle files to use the interactive visualisation as it will be much faster than training the agents from scratch. You are, of course, welcome to train the agents from scratch (as detailed above) to validate the results. To correctly load the pickle files, please follow the following steps:\n",
    "1. Run all cells above the \"Training Phase\" section.\n",
    "2. Uncomment the pickle loading code block in the cell with the comment \"SAVE OR LOAD THE PICKLE FILES HERE!!!\" and run the cell.\n",
    "3. You can now run all cells below the \"Training Phase\" section to go through the testing phase with all configurations and use the interactive visuations. However, the testing phase will take quite some time as I took an exhaustive approach to test all configurations. Since this part is only to display the performance metrics, I recommend skipping right to the *Interactive Visualisation* section and run all cells from there to see the agents in action. Instructions on how to use the interactive visualisation are included in the *Interactive Visualisation* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading pickle files to verify performance metrics:**\n",
    "In the case you do not want to run the entire notebook to verify the performance metrics, I have also included an additional pickle file storing the resulting values from the testing phase used to calculate the performance metrics. To load this pickle file, please follow the following steps:\n",
    "1. Run all cells above the \"Training Phase\" section.\n",
    "2. Uncomment the pickle loading code block in the cell with the comment \"SAVE OR LOAD THE MAIN PICKLE FILES HERE!!!\" and run the cell.\n",
    "3. Uncomment the pickle loading code block in the cell with the comment \"SAVE OR LOAD THE TEST DATA PICKLE FILES HERE!!!\", and run the cell.\n",
    "4. Run the cells right below the last one to see the value used to calculate the performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All pickle files can be downloaded from the following link: [Google Drive](https://tinyurl.com/additionalmodeltraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most packages are included in the standard Anaconda distribution, I have also decided to use *tqdm* for progress bars.\n",
    "\n",
    "This allows me to keep track of the progress of data processing, model training and evaluation.\n",
    "\n",
    "If you do not have *tqdm* installed, please kindly uncomment the code block below and run it to install the package. This is extremely lightweight and should not constitute any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum\n",
    "import torch\n",
    "import copy\n",
    "from collections import deque\n",
    "from random import shuffle, sample\n",
    "from itertools import combinations_with_replacement, product\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Note: CPU is a lot faster than GPU for this task\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Uncomment all following lines for a seeded run to reproduce one of the results detailed below\n",
    "# Set a fixed seed for reproducibility\n",
    "seed = 3210744\n",
    "\n",
    "# PyTorch seed\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# numpy and random seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_WIDTH = 5\n",
    "GRID_HEIGHT = 5\n",
    "\n",
    "# Initially, state_size was supposed to be 25 * 3 + 1 = 76. \n",
    "# However, this increased the number of parameters in the DQN by quite a bit\n",
    "# Therefore, I decided to reduce the state size to 2 * 3 + 1 = 7 to keep the training time reasonable\n",
    "# This consists of the x and y coordinates of the agent, the x and y coordinates of the opposite agent,\n",
    "# the x and y coordinates of the delivery location, and whether the agent is carrying the full secret.\n",
    "# NOTE: You can change this to 76 should you decide to test the one-hot encoding of the agent state\n",
    "STATE_SIZE = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN - Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_size, action_size = 4):\n",
    "        l1 = state_size\n",
    "        l2 = 150\n",
    "        l3 = 100\n",
    "        l4 = action_size\n",
    "        self.model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(l1, l2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(l2, l3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(l3,l4)).to(device)\n",
    "\n",
    "        self.model2 = copy.deepcopy(self.model).to(device)\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        # self.learning_rate = 0.001\n",
    "        self.learning_rate = 1e-2\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# The function \"update_target\" copies the state of the prediction network to the target network. You need to use this in regular intervals.\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "# The function \"get_qvals\" returns a numpy list of qvals for the state given by the argument based on the prediction network.\n",
    "    def get_qvals(self, state):\n",
    "        state = torch.from_numpy(state).to(device).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return q_values.cpu().detach().numpy()\n",
    "\n",
    "# The function \"get_maxQ\" returns the maximum q-value for the state given by the argument based on the target network.\n",
    "    def get_maxQ(self,state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state).to(device).float().unsqueeze(0)\n",
    "            q_values = self.model2(state)\n",
    "        return torch.max(q_values).item()\n",
    "\n",
    "# The function \"train_one_step_new\" performs a single training step.\n",
    "# It returns the current loss (only needed for debugging purposes).\n",
    "# Its parameters are three parallel lists: a minibatch of states, a minibatch of actions,\n",
    "# a minibatch of the corresponding TD targets and the discount factor.\n",
    "    def train_one_step(self, states, actions, targets):\n",
    "        targets_reply = []\n",
    "        state1_batch = torch.cat([torch.from_numpy(s).float() for s in states]).to(device)\n",
    "        action_batch = torch.Tensor(actions).to(device)\n",
    "        Q1 = self.model(state1_batch)\n",
    "        X = Q1.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze().to(device)\n",
    "        Y = torch.tensor(targets).to(device).float()\n",
    "        loss = self.loss_fn(X, Y)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action\n",
    "A simple Enum class to represent the actions that the agent can take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    # Possible agent actions\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPC - Optimal Path Calculation\n",
    "This class handles the calculation of the optimal number of steps for each possible initial configuration\n",
    "It is able to accurately calculate the theoretical optimum for both cases where the agent can wait and cannot wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OPC():\n",
    "    def __init__(self, grid_width, grid_height, num_types, num_agents_per_type, with_wait = False):\n",
    "        \"\"\"\n",
    "        The constructor of the class, which will generate all possible initial configurations\n",
    "        and find the optimal number of steps to reach the delivery point for each configuration.\n",
    "        \"\"\"\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "        # Generate all possible initial configurations\n",
    "        all_initial_configurations = OPC.generate_initial_configurations(grid_width, grid_height, num_types, num_agents_per_type)\n",
    "        progress_bar = tqdm(all_initial_configurations)\n",
    "\n",
    "        # Calculate the optimal number of steps for each configuration\n",
    "        for p in progress_bar:\n",
    "            self.results[tuple(p)] = OPC.find_steps(p, with_wait = with_wait)\n",
    "\n",
    "    def manhattan_distance(pos_1, pos_2):\n",
    "        \"\"\"\n",
    "        Calculate the manhattan distance\n",
    "        \"\"\"\n",
    "\n",
    "        x_1, y_1 = pos_1\n",
    "        x_2, y_2 = pos_2\n",
    "\n",
    "        return abs(x_2 - x_1) + abs(y_2 - y_1)\n",
    "\n",
    "    def find_adjacent(pos):\n",
    "        \"\"\"\n",
    "        Find the surrounding positions of a given position\n",
    "        \"\"\"\n",
    "\n",
    "        x, y = pos\n",
    "\n",
    "        surrounding_positions = []\n",
    "\n",
    "        for i in [-1, 1]:\n",
    "            if 0 <= x + i < GRID_WIDTH:\n",
    "                surrounding_positions.append((x + i, y))\n",
    "            if 0 <= y + i < GRID_HEIGHT:\n",
    "                surrounding_positions.append((x, y + i))\n",
    "\n",
    "        return surrounding_positions\n",
    "    \n",
    "    # Finding minimum number of steps to reach extraction point\n",
    "    def find_steps(configuration, with_wait, mode = 0):\n",
    "        \"\"\"\n",
    "        Find the number of steps to reach the delivery\n",
    "        point each possible combination of agents\n",
    "\n",
    "        \n",
    "        with_wait: A boolean value to determine whether the agent can wait (has the action WAIT)\n",
    "        mode = 0: Return all information\n",
    "        mode = 1: Return only the absolute minimum number of steps\n",
    "        \"\"\"\n",
    "        agent_1s = [(configuration[0], configuration[1]), (configuration[2], configuration[3])]\n",
    "        agent_2s = [(configuration[4], configuration[5]), (configuration[6], configuration[7])]\n",
    "\n",
    "        b = (configuration[8], configuration[9])\n",
    "\n",
    "        # List holding information on the minimum number of steps for each possible combination of agents\n",
    "        possible_num_steps = []\n",
    "\n",
    "        # Find valid adjacent positions of the delivery point\n",
    "        b_adjacent = OPC.find_adjacent(b)\n",
    "\n",
    "        for i in range(2):\n",
    "            agent_1 = agent_1s[i]\n",
    "            for j in range(2):\n",
    "                agent_2 = agent_2s[j]\n",
    "                \n",
    "                # Each element of the list will contain the following information: [b_adj_pos, agent_1_to_b_adj_pos, agent_2_to_b_adj_pos, sum_of_distances, min_steps, max_steps, case]\n",
    "                # sum_of_distances is included to determine the optimal meeting point\n",
    "                b_adj_distances = []\n",
    "\n",
    "                for b_adj_pos in b_adjacent:\n",
    "                    # Calculate the distance between the agents and the adjacent position of the delivery point\n",
    "                    agent_1_to_b_adj_pos = OPC.manhattan_distance(agent_1, b_adj_pos)\n",
    "                    agent_2_to_b_adj_pos = OPC.manhattan_distance(agent_2, b_adj_pos)\n",
    "\n",
    "                    # Case 0: If the agents are equidistant to the adjacent position of the delivery point, then the min and max steps\n",
    "                    # are the same, being the distance from an agent to the adjacent position of the delivery point + 1\n",
    "                    if agent_1_to_b_adj_pos == agent_2_to_b_adj_pos:\n",
    "                        b_adj_distances.append((b_adj_pos, agent_1_to_b_adj_pos, agent_2_to_b_adj_pos, agent_1_to_b_adj_pos + agent_2_to_b_adj_pos, agent_1_to_b_adj_pos + 1, agent_1_to_b_adj_pos + 1, 0))\n",
    "                    # Case 1: If agent 1 is further away from the adjacent position of the delivery point, then the optimal path is achieved when agent 1 goes \n",
    "                    # first and the min steps will be the distance from agent 1 to the adjacent position of the delivery point, with max steps being min steps + 1\n",
    "                    elif agent_1_to_b_adj_pos > agent_2_to_b_adj_pos:\n",
    "                        b_adj_distances.append((b_adj_pos, agent_1_to_b_adj_pos, agent_2_to_b_adj_pos, agent_1_to_b_adj_pos + agent_2_to_b_adj_pos, agent_1_to_b_adj_pos, agent_1_to_b_adj_pos + 1, 1))\n",
    "                    # Case 2: If agent 2 is further away from the adjacent position of the delivery point, then the optimal path is achieved when agent 2 goes \n",
    "                    # first and the min steps will be the distance from agent 2 to the adjacent position of the delivery point, with max steps being min steps + 1\n",
    "                    else:\n",
    "                        b_adj_distances.append((b_adj_pos, agent_1_to_b_adj_pos, agent_2_to_b_adj_pos, agent_1_to_b_adj_pos + agent_2_to_b_adj_pos, agent_2_to_b_adj_pos, agent_2_to_b_adj_pos + 1, 2))\n",
    "                    # NOTE: Case 1 and Case 2 are True and kept as is WHEN THE AGENT CAN WAIT\n",
    "\n",
    "                # Filter the b_adj_distances list based on the sum of distances. Positions with the minimum sum of distances are candidates for the optimal meeting point\n",
    "                min_sum = min([x[3] for x in b_adj_distances])\n",
    "                list_of_min_b_adj_distances = [x for x in b_adj_distances if x[3] == min_sum]\n",
    "\n",
    "                # Filter the positions with minimum sum based on the difference between the distances of the agents to the adjacent position of the delivery point\n",
    "                # Positions with the minimum difference are candidates for the optimal meeting point. In cases where there are multiple candidates, the one with index 0 is selected\n",
    "                least_difference = min([abs(x[1] - x[2]) for x in list_of_min_b_adj_distances])\n",
    "                list_of_least_difference = [x for x in list_of_min_b_adj_distances if abs(x[1] - x[2]) == least_difference]\n",
    "\n",
    "                # If there is no WAIT action, the distance between the agents is even, and it's not case 0, then the minimum number of steps is the same as the maximum number of steps\n",
    "                if with_wait == False and OPC.manhattan_distance(agent_1, agent_2) % 2 == 0 and list_of_least_difference[0][6] != 0:\n",
    "                    # (agent_1, agent_2, steps_min, steps_max, case, selected_b_adj_pos, distance between agent_1 and agent_2)\n",
    "                    possible_num_steps.append((agent_1, agent_2, list_of_least_difference[0][4] + 1, list_of_least_difference[0][5], list_of_least_difference[0][6], list_of_least_difference[0][0], OPC.manhattan_distance(agent_1, agent_2)))\n",
    "                else:\n",
    "                    possible_num_steps.append((agent_1, agent_2, list_of_least_difference[0][4], list_of_least_difference[0][5], list_of_least_difference[0][6], list_of_least_difference[0][0], OPC.manhattan_distance(agent_1, agent_2)))\n",
    "\n",
    "        # Sort based on the following:\n",
    "        # 1. Distance between agent 1 and agent 2\n",
    "        possible_num_steps.sort(key=lambda x: x[-1])\n",
    "\n",
    "        # 2. Max of steps required to reach the extraction point\n",
    "        possible_num_steps.sort(key=lambda x: x[3])\n",
    "\n",
    "        # 3. Min of steps required to reach the extraction point\n",
    "        possible_num_steps.sort(key=lambda x: x[2])\n",
    "        \n",
    "        if mode == 0:\n",
    "            return possible_num_steps\n",
    "        else:\n",
    "            return possible_num_steps[0][2]\n",
    "    \n",
    "    def find_best_agents(possible_num_steps: list):\n",
    "        \"\"\"\n",
    "        Find the best agent pair and the agent to move first\n",
    "        \"\"\"\n",
    "\n",
    "        best_agent_1 = possible_num_steps[0][0]\n",
    "        best_agent_2 = possible_num_steps[0][1]\n",
    "        agent_to_move_first = possible_num_steps[0][4]\n",
    "\n",
    "        return best_agent_1, best_agent_2, agent_to_move_first\n",
    "    \n",
    "    def print_info(possible_num_steps):\n",
    "        \"\"\"\n",
    "        Print the information of the possible number of steps\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(len(possible_num_steps)):\n",
    "            agent_1, agent_2, steps_min, steps_max, case, selected_b_adj_pos, distance = possible_num_steps[i]\n",
    "            print(f\"Agent 1: {agent_1}, Agent 2: {agent_2}, Distance: {distance}, Steps Min: {steps_min}, Steps Max: {steps_max}, Case: {case}, Selected B Adj Pos: {selected_b_adj_pos}\")\n",
    "            # print(f\"Agent 1: {agent_1}, Agent 2: {agent_2}, Steps Min: {steps_min}, Steps Max: {steps_max}, Agent to take 1st move: {case}, Selected B Adj Pos: {selected_b_adj_pos}\")\n",
    "            # print(f\"Agent 1: {agent_1}, Agent 2: {agent_2}, Distance: {distance}, Steps Min: {steps_min}, Steps Max: {steps_max}, Agent to take 1st move: {case}\")\n",
    "\n",
    "    def generate_initial_configurations(grid_width, grid_height, num_types, num_agents_per_type):\n",
    "        \"\"\"\n",
    "        This method generates all possible initial configurations without functionally equivalent configurations.\n",
    "        For example, if the config for agent 1s is (1, 2, 3, 4), then this is the same as (3, 4, 1, 2). The same applies to agent 2s.\n",
    "        This means these 4 configurations (1, 2, 3, 4, 1, 2, 3, 4, 4, 4), (3, 4, 1, 2, 3, 4, 1, 2, 4, 4), (1, 2, 3, 4, 3, 4, 1, 2, 4, 4),\n",
    "        and (3, 4, 1, 2, 1, 2, 3, 4, 4, 4) are functionally equivalent. This method will preemtively remove these configurations during generation.\n",
    "        Note that the configuration that is kept will be naturally sorted in ascending order for each agent type.\n",
    "        Overlapping positions are allowed for both same and opposite agents, but the delivery point cannot be the same as any agent's position.\n",
    "        As a result, this generates 2,250,000 configurations for a 5x5 grid with 2 agents of each type.\n",
    "        \"\"\"\n",
    "        grid_positions = [(i, j) for i in range(grid_width) for j in range(grid_height)]\n",
    "        possible_agent_positions_per_type = list(combinations_with_replacement(grid_positions, num_agents_per_type))\n",
    "        all_possible_agent_positions_of_all_types = list(product(possible_agent_positions_per_type, repeat=num_types))\n",
    "        all_possible_configurations = list(product(all_possible_agent_positions_of_all_types, grid_positions))\n",
    "\n",
    "        def flatten_element(nested_element):\n",
    "            flat_list = []\n",
    "            for item in nested_element:\n",
    "                if isinstance(item, (list, tuple)):\n",
    "                    flat_list.extend(flatten_element(item))\n",
    "                else:\n",
    "                    flat_list.append(item)\n",
    "            return flat_list\n",
    "        \n",
    "        all_initial_configurations = []\n",
    "\n",
    "        for element in all_possible_configurations:\n",
    "            flattened_element = flatten_element(element)\n",
    "\n",
    "            agent_1a = (flattened_element[0], flattened_element[1])\n",
    "            agent_1b = (flattened_element[2], flattened_element[3])\n",
    "            agent_2a = (flattened_element[4], flattened_element[5])\n",
    "            agent_2b = (flattened_element[6], flattened_element[7])\n",
    "            b = (flattened_element[8], flattened_element[9])\n",
    "\n",
    "            if agent_1a == b or agent_1b == b or agent_2a == b or agent_2b == b:\n",
    "                continue\n",
    "\n",
    "            all_initial_configurations.append(flattened_element)\n",
    "\n",
    "        return all_initial_configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2250000/2250000 [00:34<00:00, 64285.72it/s]\n"
     ]
    }
   ],
   "source": [
    "opc = OPC(GRID_WIDTH, GRID_HEIGHT, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent - Agent1 and Agent2\n",
    "The Agent class is the base class for the agents. It contains the basic structure of the agent and the methods that the agent can use to interact with the environment.\n",
    "\n",
    "Agent1 and Agent2 are the two agent types that will be used in the environment. They inherit from the Agent class but initialise a different DQN as class attribute so all agents of the same type share the same DQN.\n",
    "\n",
    "**Important Note:** In the implementation of Stage 3, the agent cannot make illegal moves in accordance to the guidance of Mr. Stephen Zhang. In other words, the agent cannot make a decision that would be the equivalent of moving into a wall or moving out of the grid. If the selected move was randomly chosen during training, the action will be rolled again until a legal move is selected. If this is the selected best action (commonly occurs at the beginning due to random initialisation), the agent will be forced to make the next best legal move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    e: float = 0.9\n",
    "    dqn: DQN\n",
    "\n",
    "    def __init__(self):\n",
    "        self.e = self.e\n",
    "\n",
    "    def reset_with_params(self, agent_x: int, agent_y: int, opposite_agent_a: \"Agent\", opposite_agent_b: \"Agent\", b_x: int, b_y: int, has_secret: bool) -> None:\n",
    "        self.agent_pos = (agent_x, agent_y)\n",
    "        self.opposite_agent_a = opposite_agent_a\n",
    "        self.opposite_agent_b = opposite_agent_b\n",
    "        self.b = (b_x, b_y)\n",
    "        self.has_secret = has_secret\n",
    "     \n",
    "    def configuration(self) -> tuple:\n",
    "        return (self.agent_pos[0], self.agent_pos[1], self.opposite_agent_a, self.opposite_agent_b, self.b[0], self.b[1], self.has_secret)\n",
    "    \n",
    "    def vector(self):\n",
    "        # Uncomment the following lines to use one-hot encoding\n",
    "        # agent_ohe = Agent.one_hot_encode(self.agent_pos[0], self.agent_pos[1], GRID_WIDTH, GRID_HEIGHT)\n",
    "        # opposite_ohe = Agent.one_hot_encode(self.opposite_pos[0], self.opposite_pos[1], GRID_WIDTH, GRID_HEIGHT)\n",
    "        # b_ohe = Agent.one_hot_encode(self.b[0], self.b[1], GRID_WIDTH, GRID_HEIGHT)\n",
    "        # has_secret = 1. if self.has_secret else 0.\n",
    "        # return np.concatenate((agent_ohe, opposite_ohe, b_ohe, np.array([has_secret])))\n",
    "\n",
    "        return np.array([self.agent_pos[0], self.agent_pos[1], self.opposite_agent_a.agent_pos[0], self.opposite_agent_a.agent_pos[1], self.opposite_agent_b.agent_pos[0], self.opposite_agent_b.agent_pos[1], self.b[0], self.b[1], 1. if self.has_secret else 0.])\n",
    "\n",
    "    # NOTE: The logic used for selecting an action was adjusted to explicitly prevent the agent from selecting illegal actions, instead of employing penalties.\n",
    "    # Otherwise, the logic to use the DQN is the same as the one used in the previous assignment.\n",
    "    def bestAction(self) -> Action:\n",
    "        q_values = self.dqn.get_qvals(self.vector()).flatten()\n",
    "\n",
    "        # Select the best action that is not illegal - this helps prevent the agent from selecting illegal actions that can happen due to initial q-values\n",
    "        for action_val in np.argsort(q_values)[::-1]:\n",
    "            action = Action(action_val)\n",
    "            if action not in self.getIllegalActions():\n",
    "                return action\n",
    "    \n",
    "    def getNextAction(self) -> Action:\n",
    "        # Epsilon essentially means \"explore\" with probability e\n",
    "        # For example, if e = 0.1, there is a 10% chance of exploring\n",
    "        if np.random.rand() < self.e:\n",
    "            while True:\n",
    "                action = Action(np.random.randint(0, len(Action)))\n",
    "                if action not in self.getIllegalActions():\n",
    "                    return action\n",
    "        else:\n",
    "            return self.bestAction()\n",
    "    \n",
    "    def getIllegalActions(self) -> list:\n",
    "        illegal_actions = []\n",
    "\n",
    "        if self.agent_pos[0] == 0:\n",
    "            illegal_actions.append(Action.LEFT)\n",
    "        if self.agent_pos[0] == GRID_WIDTH - 1:\n",
    "            illegal_actions.append(Action.RIGHT)\n",
    "        if self.agent_pos[1] == 0:\n",
    "            illegal_actions.append(Action.UP)\n",
    "        if self.agent_pos[1] == GRID_HEIGHT - 1:\n",
    "            illegal_actions.append(Action.DOWN)\n",
    "        \n",
    "        return illegal_actions\n",
    "    \n",
    "    def getQValue(self, action:Action) -> float:\n",
    "        return self.dqn.get_qvals(self.vector())[action.value]\n",
    "\n",
    "    def one_hot_encode(x, y, limit_x, limit_y):\n",
    "        ohv = np.zeros(limit_x * limit_y)\n",
    "        pos = x + y * limit_y\n",
    "        ohv[pos] = 1\n",
    "        return ohv\n",
    "    \n",
    "    def isTerminal(self) -> bool:\n",
    "        return self.agent_pos == self.b and self.has_secret\n",
    "    \n",
    "    def __hash__(self) -> int:\n",
    "        return hash(self.configuration())\n",
    "    \n",
    "    def __eq__(self, other) -> bool:\n",
    "        return self.configuration() == other.configuration()\n",
    "\n",
    "class Agent1(Agent):\n",
    "    num_instances: int = 0    \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        if Agent1.num_instances == 0:\n",
    "            Agent1.dqn = DQN(STATE_SIZE)\n",
    "\n",
    "        Agent1.num_instances += 1\n",
    "\n",
    "    # def __del__(self):\n",
    "    #     Agent1.num_instances -= 1\n",
    "\n",
    "class Agent2(Agent):\n",
    "    num_instances: int = 0    \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        if Agent2.num_instances == 0:\n",
    "            Agent2.dqn = DQN(STATE_SIZE)\n",
    "        Agent2.num_instances += 1\n",
    "\n",
    "    # def __del__(self):\n",
    "    #     Agent2.num_instances -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State\n",
    "This class is no longer directly involved in the training process. It is used to represent the full state of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State():\n",
    "    # The state class is now no longer directly used for the training of the DQN. It is more of a global representation of the environment state.\n",
    "    def __init__(self, agent_1a: Agent1, agent_1b: Agent1, agent_2a: Agent2, agent_2b: Agent2, b: tuple):\n",
    "        self.agent_1a = agent_1a\n",
    "        self.agent_1b = agent_1b\n",
    "        self.agent_2a = agent_2a\n",
    "        self.agent_2b = agent_2b\n",
    "        self.b = b\n",
    "\n",
    "    def isTerminal(self) -> bool:\n",
    "        # Determine if the agent has reached a terminal state\n",
    "        # Returns True if one agent has the secret and is at the extraction point\n",
    "        return self.agent_1a.isTerminal() or self.agent_1b.isTerminal() or self.agent_2a.isTerminal() or self.agent_2b.isTerminal()\n",
    "    \n",
    "    def configuration(self) -> tuple:\n",
    "        return (self.agent_1a.configuration(), self.agent_1b.configuration(), self.agent_2a.configuration(), self.agent_2b.configuration(), self.b)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        # Consider two states to be the same if their properties are equal\n",
    "        return (self.agent_1a.__hash__() == other.agent_1a.__hash__() and\n",
    "                self.agent_1b.__hash__() == other.agent_1b.__hash__() and\n",
    "                self.agent_2a.__hash__() == other.agent_2a.__hash__() and\n",
    "                self.agent_2b.__hash__() == other.agent_2b.__hash__() and\n",
    "                self.b == other.b)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str((self.agent_1a.agent_pos, self.agent_1b.agent_pos, self.agent_2a.agent_pos, self.agent_2b.agent_pos, self.b, (self.agent_1a.has_secret, self.agent_1b.has_secret, self.agent_2a.has_secret, self.agent_2b.has_secret)))\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.agent_1a.__hash__(), self.agent_1b.__hash__(), self.agent_2a.__hash__(), self.agent_2b.__hash__(), self.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### World\n",
    "\n",
    "The World class is effectively the environment. It contains the agents and the grid that the agents will be moving on. When the agents move, this class handles all interactions between the agents and the grid.\n",
    "\n",
    "Please note that this class constructor has some nifty logic to make use of Central Clock and the information calculated by the OPC class. Please kindly refer to the comments in the code for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class World():\n",
    "    def __init__(self):\n",
    "        # self.reset_with_params(1, 1, 0, 0, 2, 2)\n",
    "        self.reset_with_params(1, 1, 0, 0, 2, 2, 3, 3, 4, 4)\n",
    "    \n",
    "    def reset_with_params(self, agent_1a_x: int, agent_1a_y: int, agent_1b_x: int, agent_1b_y: int, agent_2a_x: int, agent_2a_y: int, agent_2b_x: int, agent_2b_y: int, b_x: int, b_y: int):\n",
    "        # Sort the agent positions to ensure they are in ascending order since the optimal path\n",
    "        # data calulated are based on the format/order generated as mentioned in the OPC class.\n",
    "        agent_1s = [(agent_1a_x, agent_1a_y), (agent_1b_x, agent_1b_y)]\n",
    "        agent_1s.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "        agent_2s = [(agent_2a_x, agent_2a_y), (agent_2b_x, agent_2b_y)]\n",
    "        agent_2s.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "        has_secret = self.checkSecret(agent_1s, agent_2s)\n",
    "\n",
    "        # Extracting optimal information for this initial configuration\n",
    "        self.initial_config = (agent_1s[0][0], agent_1s[0][1], agent_1s[1][0], agent_1s[1][1], agent_2s[0][0], agent_2s[0][1], agent_2s[1][0], agent_2s[1][1], b_x, b_y)\n",
    "        # self.min_steps = opc.results[self.initial_config][0][2]\n",
    "        self.min_steps = opc.results[self.initial_config][0][3]\n",
    "        self.best_meetup_pos = opc.results[self.initial_config][0][5]\n",
    "        self.best_agent_1 = opc.results[self.initial_config][0][0]\n",
    "        self.best_agent_2 = opc.results[self.initial_config][0][1]\n",
    "        self.agent_to_move_first = opc.results[self.initial_config][0][4]\n",
    "        self.best_agent_1_index = agent_1s.index(self.best_agent_1)\n",
    "        self.best_agent_2_index = agent_2s.index(self.best_agent_2)\n",
    "\n",
    "        # Creating the agents\n",
    "        self.agent_1a = Agent1()\n",
    "        self.agent_1b = Agent1()\n",
    "        self.agent_2a = Agent2()\n",
    "        self.agent_2b = Agent2()\n",
    "\n",
    "        self.agents = [self.agent_1a, self.agent_1b, self.agent_2a, self.agent_2b]\n",
    "\n",
    "\n",
    "        # NOTE NOTE NOTE: As previously discussed and allowed by Professor Meyer, I am initialising the world to always have agent 1a and agent 2a\n",
    "        # be a pair of agents that has the potential to reach the extraction point in the minimum number of steps. This allows a quick and easy way \n",
    "        # to design the optimal agent sequence for each world configuration as can be seen later in the training and testing sections.\n",
    "\n",
    "        # TODO: If there are more than 2 agents of each type, the following implementation should be changed to allow better agent instantiation\n",
    "        # Agent 1a\n",
    "        # closest_opposite_agent_pos = min(agent_2s, key=lambda x: OPC.manhattan_distance(self.best_agent_1, x)\n",
    "        # self.agent_1a.reset_with_params(self.best_agent_1[0], self.best_agent_1[1], closest_opposite_agent_pos[0], closest_opposite_agent_pos[1], b_x, b_y, has_secret[self.best_agent_1_index])\n",
    "        self.agent_1a.reset_with_params(self.best_agent_1[0], self.best_agent_1[1], self.agent_2a, self.agent_2b, b_x, b_y, has_secret[self.best_agent_1_index])\n",
    "\n",
    "        # Agent 1b\n",
    "        # closest_opposite_agent_pos = min(agent_2s, key=lambda x: OPC.manhattan_distance(agent_1s[int(not self.best_agent_1_index)], x))\n",
    "        # self.agent_1b.reset_with_params(agent_1s[int(not self.best_agent_1_index)][0], agent_1s[int(not self.best_agent_1_index)][1], closest_opposite_agent_pos[0], closest_opposite_agent_pos[1], b_x, b_y, has_secret[int(not self.best_agent_1_index)])\n",
    "        self.agent_1b.reset_with_params(agent_1s[int(not self.best_agent_1_index)][0], agent_1s[int(not self.best_agent_1_index)][1], self.agent_2a, self.agent_2b, b_x, b_y, has_secret[int(not self.best_agent_1_index)])\n",
    "\n",
    "        # Agent 2a\n",
    "        # closest_opposite_agent_pos = min(agent_1s, key=lambda x: OPC.manhattan_distance(self.best_agent_2, x))\n",
    "        # self.agent_2a.reset_with_params(self.best_agent_2[0], self.best_agent_2[1], closest_opposite_agent_pos[0], closest_opposite_agent_pos[1], b_x, b_y, has_secret[self.best_agent_2_index + 2])\n",
    "        # self.agent_2a.reset_with_params(self.best_agent_2[0], self.best_agent_2[1], closest_opposite_agents_pos[0][0], closest_opposite_agents_pos[0][1], closest_opposite_agents_pos[1][0], closest_opposite_agents_pos[1][1], b_x, b_y, has_secret[self.best_agent_2_index + 2])\n",
    "        self.agent_2a.reset_with_params(self.best_agent_2[0], self.best_agent_2[1], self.agent_1a, self.agent_1b, b_x, b_y, has_secret[self.best_agent_2_index + 2])\n",
    "\n",
    "        # Agent 2b\n",
    "        # closest_opposite_agent_pos = min(agent_1s, key=lambda x: OPC.manhattan_distance(agent_2s[int(not self.best_agent_2_index)], x))\n",
    "        # closest_opposite_agents_pos = sorted(agent_1s, key=lambda x: OPC.manhattan_distance(agent_2s[int(not self.best_agent_2_index)], x))\n",
    "        # self.agent_2b.reset_with_params(agent_2s[int(not self.best_agent_2_index)][0], agent_2s[int(not self.best_agent_2_index)][1], closest_opposite_agent_pos[0], closest_opposite_agent_pos[1], b_x, b_y, has_secret[int(not self.best_agent_2_index) + 2])\n",
    "        # self.agent_2b.reset_with_params(agent_2s[int(not self.best_agent_2_index)][0], agent_2s[int(not self.best_agent_2_index)][1], closest_opposite_agents_pos[0][0], closest_opposite_agents_pos[0][1], closest_opposite_agents_pos[1][0], closest_opposite_agents_pos[1][1], b_x, b_y, has_secret[int(not self.best_agent_2_index) + 2])\n",
    "        self.agent_2b.reset_with_params(agent_2s[int(not self.best_agent_2_index)][0], agent_2s[int(not self.best_agent_2_index)][1], self.agent_1a, self.agent_1b, b_x, b_y, has_secret[int(not self.best_agent_2_index) + 2])\n",
    "\n",
    "        self.state = State(self.agent_1a, self.agent_1b, self.agent_2a, self.agent_2b, (b_x, b_y))\n",
    "\n",
    "    def applyAction(self, agent:Agent, action:Action) -> None:\n",
    "        \"\"\"\n",
    "        Apply the action to the agent. State will be automatically updated since it is passed by reference.\n",
    "        \"\"\"\n",
    "        # Apply action based on action type and return the new state\n",
    "        match action:\n",
    "            case Action.UP:\n",
    "                new_y = agent.agent_pos[1] - 1 # avoid going out the top of the grid\n",
    "                agent.agent_pos = (agent.agent_pos[0], new_y)\n",
    "            case Action.DOWN:\n",
    "                new_y = agent.agent_pos[1] + 1 # avoid going out the bottom of the grid\n",
    "                agent.agent_pos = (agent.agent_pos[0], new_y)\n",
    "            case Action.LEFT:\n",
    "                new_x = agent.agent_pos[0] - 1 # avoid going out the left of the grid\n",
    "                agent.agent_pos = (new_x, agent.agent_pos[1])\n",
    "            case Action.RIGHT:\n",
    "                new_x = agent.agent_pos[0] + 1 # avoid going out the right of the grid\n",
    "                agent.agent_pos = (new_x, agent.agent_pos[1])\n",
    "\n",
    "        # Update has_secret for the agent and any opposite agents that are at the same position\n",
    "        self.updateSecret(agent)\n",
    "    \n",
    "    def updateSecret(self, agent) -> None:\n",
    "        \"\"\"\n",
    "        This function checks and updates the has_secret property of the agent and any opposite agents that are at the same position\n",
    "        \"\"\"\n",
    "\n",
    "        if agent.agent_pos != self.state.b:\n",
    "            if isinstance(agent, Agent1):\n",
    "                for a in [self.agent_2a, self.agent_2b]:\n",
    "                    if agent.agent_pos == a.agent_pos:\n",
    "                        agent.has_secret = True\n",
    "                        a.has_secret = True\n",
    "            elif isinstance(agent, Agent2):\n",
    "                for a in [self.agent_1a, self.agent_1b]:\n",
    "                    if agent.agent_pos == a.agent_pos:\n",
    "                        agent.has_secret = True\n",
    "                        a.has_secret = True\n",
    "\n",
    "    def checkSecret(self, agent_1s, agent_2s) -> list:\n",
    "        # An agent will carry the secret if they are at the same position as an agent of the other type\n",
    "        has_secret = [False, False, False, False]\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                if agent_1s[i] == agent_2s[j]:\n",
    "                    has_secret[i] = True\n",
    "                    has_secret[j+2] = True\n",
    "\n",
    "        return has_secret\n",
    "\n",
    "    \n",
    "    def getReward(self, old_agent: Agent, new_agent: Agent) -> int:\n",
    "        # While the reward seems disproportionately high, it actually helps form a stronger gradient signals for backpropagation,\n",
    "        # leading to much faster convergence and more effective updates to the Q-network weights.\n",
    "\n",
    "        # This effectively means if the agent has the secret and is at the extraction point, it will receive a reward of 500\n",
    "        if new_agent.isTerminal():\n",
    "            # Reached B with package\n",
    "            return 1000\n",
    "        \n",
    "        \n",
    "        # If agent has picked up the secret (previously did not have it and just picked it up)\n",
    "        if old_agent.has_secret == False and new_agent.has_secret == True:\n",
    "            # Agent has picked up the secret - aka have met an agent of the other type\n",
    "            return 500\n",
    "    \n",
    "        # Penalise for each step\n",
    "        return -10\n",
    "\n",
    "    def closest_opposite_agent(self, agent:Agent) -> Agent:\n",
    "        if isinstance(agent, Agent1):\n",
    "            return min([self.agent_2a, self.agent_2b], key=lambda x: OPC.manhattan_distance(agent.agent_pos, x.agent_pos))\n",
    "        elif isinstance(agent, Agent2):\n",
    "            return min([self.agent_1a, self.agent_1b], key=lambda x: OPC.manhattan_distance(agent.agent_pos, x.agent_pos))\n",
    "        \n",
    "    def closest_opposite_agents(self, agent:Agent) -> list:\n",
    "        if isinstance(agent, Agent1):\n",
    "            return sorted([self.agent_2a, self.agent_2b], key=lambda x: OPC.manhattan_distance(agent.agent_pos, x.agent_pos))\n",
    "        elif isinstance(agent, Agent2):\n",
    "            return sorted([self.agent_1a, self.agent_1b], key=lambda x: OPC.manhattan_distance(agent.agent_pos, x.agent_pos))\n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        # Print ASCII representation of the grid\n",
    "\n",
    "        # New version\n",
    "        output = \"\"\n",
    "        for y in range(GRID_HEIGHT):\n",
    "            for x in range(GRID_WIDTH):\n",
    "                # If agent of type 1 and 2 are at the same position (x, y)\n",
    "                # if (x, y) in self.state.agent_1s and (x, y) in self.state.agent_2s:\n",
    "                if (x, y) in [self.agent_1a.agent_pos, self.agent_1b.agent_pos] and (x, y) in [self.agent_2a.agent_pos, self.agent_2b.agent_pos]:\n",
    "                    output += \" @\"\n",
    "                # If both agents of type 1 are at position (x, y)\n",
    "                # elif (self.state.agent_1s[0] == (x, y) and self.state.agent_1s[1] == (x, y)):\n",
    "                elif (x, y) == self.agent_1a.agent_pos and (x, y) == self.agent_1b.agent_pos:\n",
    "                    output += \" #\"\n",
    "                # If both agents of type 2 are at position (x, y)\n",
    "                # elif (self.state.agent_2s[0] == (x, y) and self.state.agent_2s[1] == (x, y)):\n",
    "                elif (x, y) == self.agent_2a.agent_pos and (x, y) == self.agent_2b.agent_pos:\n",
    "                    output += \" $\"\n",
    "                # If agent of type 1 is at position (x, y) and does not have the secret\n",
    "                # elif (self.state.agent_1s[0] == (x, y) and self.state.has_secret[0] == False) or (self.state.agent_1s[1] == (x, y) and self.state.has_secret[1]  == False):\n",
    "                elif (x, y) == self.agent_1a.agent_pos and not self.agent_1a.has_secret or (x, y) == self.agent_1b.agent_pos and not self.agent_1b.has_secret:\n",
    "                    output += \" a\"\n",
    "                # If agent of type 2 is at position (x, y) and does not have the secret\n",
    "                # elif (self.state.agent_2s[0] == (x, y) and self.state.has_secret[2] == False) or (self.state.agent_2s[1] == (x, y) and self.state.has_secret[3] == False):\n",
    "                elif (x, y) == self.agent_2a.agent_pos and not self.agent_2a.has_secret or (x, y) == self.agent_2b.agent_pos and not self.agent_2b.has_secret:\n",
    "                    output += \" b\"\n",
    "                # If agent of type 1 is at position (x, y) and has the secret\n",
    "                # elif (self.state.agent_1s[0] == (x, y) and self.state.has_secret[0] == True) or (self.state.agent_1s[1] == (x, y) and self.state.has_secret[1] == True):\n",
    "                elif (x, y) == self.agent_1a.agent_pos and self.agent_1a.has_secret or (x, y) == self.agent_1b.agent_pos and self.agent_1b.has_secret:\n",
    "                    output += \" A\"\n",
    "                # If agent of type 2 is at position (x, y) and has the secret\n",
    "                # elif (self.state.agent_2s[0] == (x, y) and self.state.has_secret[2] == True) or (self.state.agent_2s[1] == (x, y) and self.state.has_secret[3] == True):\n",
    "                elif (x, y) == self.agent_2a.agent_pos and self.agent_2a.has_secret or (x, y) == self.agent_2b.agent_pos and self.agent_2b.has_secret:\n",
    "                    output += \" B\"\n",
    "                # If extraction point is at position (x, y)\n",
    "                elif self.state.b == (x, y):\n",
    "                    output += \" β\"\n",
    "                else:\n",
    "                    output += \" -\"\n",
    "                \n",
    "            output += \"\\n\"\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    \"\"\"\n",
    "    Saves and measures performance metrics of the agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, steps_patience = GRID_HEIGHT * GRID_WIDTH * 2):\n",
    "        self.metric_values = {\"loss_1\" : {}, \"loss_2\" : {}, \"average_excess_steps\" : {}, \"success_rate\" : {}, \"average_cumulated_rewards\" : {}}\n",
    "        self.steps_patience = steps_patience\n",
    "\n",
    "    def save_loss_1(self, episode, loss):\n",
    "        \"\"\"\n",
    "        Save loss during training episode\n",
    "        \"\"\"\n",
    "        self.metric_values[\"loss_1\"][episode] = loss\n",
    "\n",
    "    def save_loss_2(self, episode, loss):\n",
    "        \"\"\"\n",
    "        Save loss during training episode\n",
    "        \"\"\"\n",
    "        self.metric_values[\"loss_2\"][episode] = loss\n",
    "\n",
    "    def display_metrics(self, metrics_name, ylabel, xlabel=\"Episode\"):\n",
    "        metrics_array = self.get_metrics(metrics_name)\n",
    "\n",
    "        episodes = metrics_array[:, 0]\n",
    "        metric_values = metrics_array[:, 1]\n",
    "\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(episodes, metric_values)\n",
    "\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def get_metrics(self, metrics_name):\n",
    "        return np.array(list(self.metric_values[metrics_name].items()))\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def save_metrics(self, episode, config_set):\n",
    "        \"\"\"\n",
    "        Evaluates the model on a number of test cases. The number of test cases is givenb by the validation_size argument.\n",
    "        \"\"\"\n",
    "        # Keep list of initial states already used\n",
    "        total_excess_steps = 0\n",
    "        success_count = 0 # Count the number of times the agent succesfully reaches the goal\n",
    "        total_cumulated_rewards = 0\n",
    "\n",
    "        for agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y in config_set:\n",
    "            if ((agent_1a_x, agent_1a_y) == (b_x, b_y) or \n",
    "                (agent_1b_x, agent_1b_y) == (b_x, b_y) or \n",
    "                (agent_2a_x, agent_2a_y) == (b_x, b_y) or \n",
    "                (agent_2b_x, agent_2b_y) == (b_x, b_y)):\n",
    "                continue\n",
    "\n",
    "            world.reset_with_params(agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y)\n",
    "            \n",
    "            i = 0\n",
    "            while not world.state.isTerminal() and i < GRID_HEIGHT * GRID_WIDTH * 2:\n",
    "                if world.agent_to_move_first == 1:\n",
    "                    agents = [world.agent_1a, world.agent_1b, world.agent_2a, world.agent_2b]\n",
    "                else:\n",
    "                    agents = [world.agent_2a, world.agent_2b, world.agent_1a, world.agent_1b]\n",
    "\n",
    "                for agent in agents:\n",
    "                    # agent.opposite_pos = world.closest_opposite_agent(agent).agent_pos\n",
    "                    action = agent.bestAction()\n",
    "                    old_agent = copy.deepcopy(agent)\n",
    "                    world.applyAction(agent, action)\n",
    "                    total_cumulated_rewards += world.getReward(old_agent, agent)\n",
    "                    if world.state.isTerminal():\n",
    "                        break\n",
    "                i += 1\n",
    "        \n",
    "            # Calculate excess number of steps\n",
    "            total_excess_steps +=  i - world.min_steps\n",
    "\n",
    "            # Calculate the number of succeses\n",
    "            if i < self.steps_patience:\n",
    "                success_count += 1\n",
    "        \n",
    "        # Calcualte average number of excess steps and save\n",
    "        average_excess_steps = total_excess_steps / len(config_set)\n",
    "        self.metric_values[\"average_excess_steps\"][episode] = average_excess_steps\n",
    "\n",
    "        # Calculate success rate and save\n",
    "        self.metric_values[\"success_rate\"][episode] = (success_count/len(config_set))*100\n",
    "\n",
    "        # Calculate average \n",
    "        self.metric_values[\"average_cumulated_rewards\"][episode] = total_cumulated_rewards/len(config_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Algorithm Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 # Exploration rate\n",
    "min_epsilon = 0.1\n",
    "gamma = 0.95 # Discount factor\n",
    "\n",
    "# Initialise world and agent\n",
    "world = World()\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_pos = {\n",
    "    (0, 0): [],\n",
    "    (0, 1): [],\n",
    "    (0, 2): [],\n",
    "    (0, 3): [],\n",
    "    (0, 4): [],\n",
    "    (1, 0): [],\n",
    "    (1, 1): [],\n",
    "    (1, 2): [],\n",
    "    (1, 3): [],\n",
    "    (1, 4): [],\n",
    "    (2, 0): [],\n",
    "    (2, 1): [],\n",
    "    (2, 2): [],\n",
    "    (2, 3): [],\n",
    "    (2, 4): [],\n",
    "    (3, 0): [],\n",
    "    (3, 1): [],\n",
    "    (3, 2): [],\n",
    "    (3, 3): [],\n",
    "    (3, 4): [],\n",
    "    (4, 0): [],\n",
    "    (4, 1): [],\n",
    "    (4, 2): [],\n",
    "    (4, 3): [],\n",
    "    (4, 4): []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19999"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_configs = list(opc.results.keys())\n",
    "shuffle(all_configs)\n",
    "\n",
    "for config in all_configs:\n",
    "    if len(b_pos[config[8], config[9]]) < 800:\n",
    "        b_pos[config[8], config[9]].append(config)\n",
    "\n",
    "b_pos[(4, 4)].pop(0)\n",
    "\n",
    "sum([len(v) for k, v in b_pos.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_training_configs = []\n",
    "for k, v in b_pos.items():\n",
    "    final_training_configs.extend(v)\n",
    "\n",
    "len(final_training_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent before Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows how the Agent attempts to complete the task before it has had any learning. This test terminates if the agent has not reached the terminal state (position B after going to position A) within GRID_HEIGHT * GRID_WIDTH * 2 steps. \n",
    "\n",
    "**This test will be repeated after the training is completed.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a - - - -\n",
      " - a - - -\n",
      " - - b - -\n",
      " - - - b -\n",
      " - - - - β\n",
      "\n",
      " - a - - -\n",
      " - - a - -\n",
      " - - - - -\n",
      " - - b - -\n",
      " - - - b β\n",
      "\n",
      " - - a - -\n",
      " - - - a -\n",
      " - - - - -\n",
      " - - - - -\n",
      " - - b - b\n",
      "\n",
      " - - - a -\n",
      " - - - - a\n",
      " - - - - -\n",
      " - - - - b\n",
      " - - - b β\n",
      "\n",
      " - - - - a\n",
      " - - - - -\n",
      " - - - - a\n",
      " - - - - -\n",
      " - - - - $\n",
      "\n",
      " - - - - -\n",
      " - - - - a\n",
      " - - - - -\n",
      " - - - - @\n",
      " - - - - β\n",
      "\n",
      " - - - - -\n",
      " - - - - a\n",
      " - - - - -\n",
      " - - - - $\n",
      " - - - - A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world.reset_with_params(0, 0, 1, 1, 2, 2, 3, 3, 4, 4)\n",
    "print(world)\n",
    "i = 0 \n",
    "\n",
    "# i < GRID_HEIGHT * GRID_WIDTH * 2 is used to prevent infinite loops\n",
    "while not world.state.isTerminal() and i < GRID_HEIGHT * GRID_WIDTH * 2:\n",
    "    # bestAction is used here to exploit the learned Q since we are not training and thus no need to explore\n",
    "    # Assuming only using best pair of agents\n",
    "    if world.agent_to_move_first == 1:\n",
    "        agents = [world.agent_1a, world.agent_1b, world.agent_2a, world.agent_2b]\n",
    "    else:\n",
    "        agents = [world.agent_2a, world.agent_2b, world.agent_1a, world.agent_1b]\n",
    "\n",
    "    for agent in agents:\n",
    "        # agent.opposite_pos = world.closest_opposite_agent(agent).agent_pos\n",
    "        # Best action from internal state representation\n",
    "        action = agent.bestAction()\n",
    "        world.applyAction(agent, action)\n",
    "        if world.state.isTerminal():\n",
    "            break\n",
    "\n",
    "    print(world)\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNfjScQlFzDt"
   },
   "source": [
    "# Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_history = {}\n",
    "episodes = 0\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "ring_buffer_1 = {'state': deque(maxlen=BUFFER_SIZE), 'action': deque(maxlen=BUFFER_SIZE), 'td': deque(maxlen=BUFFER_SIZE)}\n",
    "ring_buffer_2 = {'state': deque(maxlen=BUFFER_SIZE), 'action': deque(maxlen=BUFFER_SIZE), 'td': deque(maxlen=BUFFER_SIZE)}\n",
    "\n",
    "steps = 0\n",
    "mini_batch_size = 200\n",
    "\n",
    "loss_1 = []\n",
    "loss_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19999/19999 [14:59<00:00, 22.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# We select the final training configurations chosen earlier as the permutations to train the agents\n",
    "selected_permutations = final_training_configs\n",
    "permutations = selected_permutations\n",
    "\n",
    "# Generate a configuration subset for training evaluation metrics\n",
    "train_eval_config_set = sample(final_training_configs, 1000)\n",
    "\n",
    "shuffle(permutations) # train in randomised order\n",
    "\n",
    "# No episilon decay is used\n",
    "decay_rate = (0.1 / epsilon) ** (1 / len(permutations))\n",
    "\n",
    "Agent.e = epsilon\n",
    "\n",
    "pbar = tqdm(permutations, desc=\"Training\", total=len(permutations))\n",
    "\n",
    "for agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y in pbar:\n",
    "    # Making sure are not intantiated in the same location as the delivery point. This will not happen in this implementation\n",
    "    # but it is good to have this check in place in case complete randomisation is used in the future.\n",
    "    if ((agent_1a_x, agent_1a_y) == (b_x, b_y) or \n",
    "        (agent_1b_x, agent_1b_y) == (b_x, b_y) or \n",
    "        (agent_2a_x, agent_2a_y) == (b_x, b_y) or \n",
    "        (agent_2b_x, agent_2b_y) == (b_x, b_y)):\n",
    "        continue\n",
    "\n",
    "    world.reset_with_params(agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y)\n",
    "    \n",
    "    # Store the history (state, agent_state, action, current_step, current_sub_step) for each episode. This will be used for the interactive visualisation\n",
    "    episode_history[episodes] = {\n",
    "        \"initial_config\": (agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y),\n",
    "        \"history\": []\n",
    "    }\n",
    "\n",
    "    # If the agent to move first is 1, the agents will be in the following order: 1a, 2a, 1b, 2b\n",
    "    if world.agent_to_move_first == 1:\n",
    "        agents = [world.agent_1a, world.agent_1b, world.agent_2a, world.agent_2b]\n",
    "    # If the agent to move first is 2, the agents will be in the following order: 2a, 1a, 2b, 1b\n",
    "    else:\n",
    "        agents = [world.agent_2a, world.agent_2b, world.agent_1a, world.agent_1b]\n",
    "    # NOTE: The ordering logic above will also be used for testing to ensure the optimal agent sequence is used for each configuration\n",
    "\n",
    "    current_episode_step = 0\n",
    "\n",
    "    while not world.state.isTerminal():\n",
    "        for i, agent in enumerate(agents):\n",
    "            # Update agent's closest opposite agent\n",
    "            # agent.opposite_pos = world.closest_opposite_agent(agent).agent_pos\n",
    "\n",
    "            # Determine the next action to take\n",
    "            action = agent.getNextAction()\n",
    "\n",
    "            # Perform the action and move to the new state\n",
    "            old_agent = copy.deepcopy(agent)\n",
    "            world.applyAction(agent, action)\n",
    "\n",
    "            # Calculate the reward and the TD error\n",
    "            reward = world.getReward(old_agent, agent)\n",
    "\n",
    "            if agent.isTerminal():\n",
    "                td_target = reward\n",
    "            else:\n",
    "                td_target = reward + gamma * agent.dqn.get_maxQ(agent.vector())\n",
    "\n",
    "            # Store step in ring buffer\n",
    "            if isinstance(agent, Agent1):\n",
    "                ring_buffer = ring_buffer_1\n",
    "            else:\n",
    "                ring_buffer = ring_buffer_2\n",
    "\n",
    "            ring_buffer['state'].append(np.array([old_agent.vector()]))\n",
    "            ring_buffer['action'].append(action.value)\n",
    "            ring_buffer['td'].append(td_target)\n",
    "\n",
    "            # Store the history for this episode\n",
    "            episode_history[episodes][\"history\"].append((world.state.configuration(), agent.configuration(), action.value, current_episode_step, i))\n",
    "\n",
    "            if world.state.isTerminal():\n",
    "                break\n",
    "        \n",
    "        current_episode_step += 1\n",
    "        steps += 1\n",
    "    \n",
    "    # The ring_buffers might have different lengths since the episode might end early when one agent reaches the extraction point\n",
    "    if len(ring_buffer_1['state']) >= mini_batch_size and len(ring_buffer_2['state']) >= mini_batch_size:\n",
    "        # Sample mini-batch from ring buffer\n",
    "        idx_1 = np.random.choice(range(len(ring_buffer_1['state'])), mini_batch_size, replace=False)\n",
    "        state_batch_1 = [ring_buffer_1['state'][i] for i in idx_1]\n",
    "        action_batch_1 = [ring_buffer_1['action'][i] for i in idx_1]\n",
    "        td_batch_1 = [ring_buffer_1['td'][i] for i in idx_1]\n",
    "        batch_loss_1 = Agent1.dqn.train_one_step(state_batch_1, action_batch_1, td_batch_1)\n",
    "        loss_1.append(batch_loss_1)\n",
    "\n",
    "        idx_2 = np.random.choice(range(len(ring_buffer_2['state'])), mini_batch_size, replace=False)\n",
    "        state_batch_2 = [ring_buffer_2['state'][i] for i in idx_2]\n",
    "        action_batch_2 = [ring_buffer_2['action'][i] for i in idx_2]\n",
    "        td_batch_2 = [ring_buffer_2['td'][i] for i in idx_2]\n",
    "        batch_loss_2 = Agent2.dqn.train_one_step(state_batch_2, action_batch_2, td_batch_2)\n",
    "        loss_2.append(batch_loss_2)\n",
    "\n",
    "        # Save loss every 25 episodes - the ring buffer will have exceeded the mini-batch size by the first 25 episodes\n",
    "        if episodes % 25 == 0:\n",
    "            metrics.save_loss_1(episodes, batch_loss_1)\n",
    "            metrics.save_loss_2(episodes, batch_loss_2)\n",
    "\n",
    "        # Update the target network every 500 steps\n",
    "        if steps > 500:\n",
    "            Agent1.dqn.update_target()\n",
    "            Agent2.dqn.update_target()\n",
    "            steps = 0\n",
    "    \n",
    "    # Every 500 episodes, perform testing on a sample of 1000 configurations previously selected\n",
    "    # Depending on the size of the training set, this can slow down the training process marginally\n",
    "    # if episodes % 500 == 0:\n",
    "    #         metrics.save_metrics(episodes, train_eval_config_set)\n",
    "\n",
    "    # Agent.e = Agent.e * decay_rate\n",
    "\n",
    "    episodes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: SAVE OR LOAD THE MAIN PICKLE FILES HERE!!!\n",
    "# Save the models\n",
    "with open(f\"{seed}_Agent_1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Agent1.dqn, f)\n",
    "\n",
    "with open(f\"{seed}_Agent_2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Agent2.dqn, f)\n",
    "\n",
    "# Save other training metrics\n",
    "with open(f\"{seed}_metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metrics, f)\n",
    "\n",
    "with open(f\"{seed}_episode_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(episode_history, f)\n",
    "\n",
    "# # Load the models\n",
    "# with open(f\"{seed}_Agent_1.pkl\", \"rb\") as f:\n",
    "#     Agent1.dqn = pickle.load(f)\n",
    "\n",
    "# with open(f\"{seed}_Agent_2.pkl\", \"rb\") as f:\n",
    "#     Agent2.dqn = pickle.load(f)\n",
    "\n",
    "# # Load other training metrics\n",
    "# with open(f\"{seed}_metrics.pkl\", \"rb\") as f:\n",
    "#     metrics = pickle.load(f)\n",
    "\n",
    "# with open(f\"{seed}_episode_history.pkl\", \"rb\") as f:\n",
    "#     episode_history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training has been completed, we now repeat the same test from before. The agent is now able to complete the task in exactly the optimal/minimum number of moves, showing that it has learned how to solve the task.\n",
    "\n",
    "**Note**: The agent will not always solve the task optimally, but in this configuration, it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " β - - - -\n",
      " - - - - -\n",
      " - - - - -\n",
      " - - - - #\n",
      " - - - - $\n",
      "\n",
      "Optimal path length:  8\n",
      " β - - - -\n",
      " - - - - -\n",
      " - - - - #\n",
      " - - - - $\n",
      " - - - - -\n",
      "\n",
      " β - - - -\n",
      " - - - - #\n",
      " - - - - $\n",
      " - - - - -\n",
      " - - - - -\n",
      "\n",
      " β - - - -\n",
      " - - - # -\n",
      " - - - $ -\n",
      " - - - - -\n",
      " - - - - -\n",
      "\n",
      " β - - - -\n",
      " - - # $ -\n",
      " - - - - -\n",
      " - - - - -\n",
      " - - - - -\n",
      "\n",
      " β - - $ -\n",
      " - # - - -\n",
      " - - - - -\n",
      " - - - - -\n",
      " - - - - -\n",
      "\n",
      " β - $ - -\n",
      " # - - - -\n",
      " - - - - -\n",
      " - - - - -\n",
      " - - - - -\n",
      "\n",
      " A $ - - -\n",
      " A - - - -\n",
      " - - - - -\n",
      " - - - - -\n",
      " - - - - -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world.reset_with_params(4, 3, 4, 3, 4, 4, 4, 4, 0, 0)\n",
    "print(world)\n",
    "i = 0 \n",
    "\n",
    "print(\"Optimal path length: \", world.min_steps)\n",
    "\n",
    "# i < GRID_HEIGHT * GRID_WIDTH * 2 is used to prevent infinite loops\n",
    "while not world.state.isTerminal() and i < GRID_HEIGHT * GRID_WIDTH * 2:\n",
    "    # bestAction is used here to exploit the learned Q since we are not training and thus no need to explore\n",
    "    # Assuming only using best pair of agents\n",
    "    if world.agent_to_move_first == 1:\n",
    "        agents = [world.agent_1a, world.agent_1b, world.agent_2a, world.agent_2b]\n",
    "    else:\n",
    "        agents = [world.agent_2a, world.agent_2b, world.agent_1a, world.agent_1b]\n",
    "\n",
    "    for agent in agents:\n",
    "        # agent.opposite_pos = world.closest_opposite_agent(agent).agent_pos\n",
    "        # Best action from internal state representation\n",
    "        action = agent.bestAction()\n",
    "        world.applyAction(agent, action)\n",
    "        if world.state.isTerminal():\n",
    "            break\n",
    "\n",
    "    print(world)\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the agents on ALL possible configurations. \n",
    "\n",
    "**Note:** While there are ways to optimise this process by testing on only 1 pair of agents at a time, I have chosen to test on all possible configurations for an exhaustive performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2250000/2250000 [23:51<00:00, 1571.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# The commented-out permutation below was used to test the agents on the full set of previously extracted configurations (stored in all_count_set). This gave a rough idea of how well the agents were \n",
    "# performing on this small subset of configurations. This was heavily used to tune the hyperparameters and ensure the agents are learning effectively during the experimentation phase of this assignment.\n",
    "# It is left here for reference purposes, but it is not used in the final testing phase as it is not fully representative of the agents' performance on the full set of configurations.\n",
    "# permutations = list(all_count_set)\n",
    "\n",
    "# This set of permutations is all possible configurations that do not involve the agents starting at the extraction point. All functionally equivalent configurations are removed (for more information on this\n",
    "# please kindly review the generate_initial_configurations() method of the OPC class))\n",
    "permutations = list(opc.results.keys())\n",
    "\n",
    "test_progress = tqdm(permutations, desc=\"Testing\", total=len(permutations))\n",
    "\n",
    "num_optimal = 0\n",
    "num_suboptimal = 0\n",
    "\n",
    "list_most_optimal = []\n",
    "list_optimal = []\n",
    "list_suboptimal = []\n",
    "\n",
    "num_steps_list = []\n",
    "total_excess_steps = 0\n",
    "\n",
    "num_failed = 0\n",
    "\n",
    "skipped = 0\n",
    "\n",
    "for agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y in test_progress:\n",
    "    if ((agent_1a_x, agent_1a_y) == (b_x, b_y) or \n",
    "        (agent_1b_x, agent_1b_y) == (b_x, b_y) or \n",
    "        (agent_2a_x, agent_2a_y) == (b_x, b_y) or \n",
    "        (agent_2b_x, agent_2b_y) == (b_x, b_y)):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    world.reset_with_params(agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y)\n",
    "    \n",
    "    i = 0\n",
    "    while not world.state.isTerminal() and i < GRID_HEIGHT * GRID_WIDTH * 2:\n",
    "        if world.agent_to_move_first == 1:\n",
    "            agents = [world.agent_1a, world.agent_1b, world.agent_2a, world.agent_2b]\n",
    "        else:\n",
    "            agents = [world.agent_2a, world.agent_2b, world.agent_1a, world.agent_1b]\n",
    "\n",
    "        for agent in agents:\n",
    "            # agent.opposite_pos = world.closest_opposite_agent(agent).agent_pos\n",
    "            action = agent.bestAction()\n",
    "            world.applyAction(agent, action)\n",
    "            if world.state.isTerminal():\n",
    "                break\n",
    "        i += 1\n",
    "\n",
    "    if i < world.min_steps:\n",
    "        num_optimal += 1\n",
    "        list_most_optimal.append((agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y))\n",
    "    elif i == world.min_steps:\n",
    "        num_optimal += 1\n",
    "        list_optimal.append((agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y))\n",
    "    elif i > world.min_steps:\n",
    "        list_suboptimal.append((agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y))\n",
    "        if i >= GRID_HEIGHT * GRID_WIDTH * 2:\n",
    "            num_failed += 1\n",
    "            print(agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y)\n",
    "        else:\n",
    "            num_suboptimal += 1\n",
    "            total_excess_steps += i - world.min_steps\n",
    "            num_steps_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: SAVE OR LOAD THE TEST DATA PICKLE FILES HERE!!!\n",
    "# Save the test data\n",
    "test_data = {\n",
    "    \"num_optimal\": num_optimal,\n",
    "    \"num_suboptimal\": num_suboptimal,\n",
    "    \"num_failed\": num_failed,\n",
    "    \"total_excess_steps\": total_excess_steps,\n",
    "    \"num_steps_list\": num_steps_list,\n",
    "    \"skipped\": skipped\n",
    "}\n",
    "\n",
    "with open(f\"{seed}_test_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_data, f)\n",
    "\n",
    "# # Load the test data\n",
    "# with open(f\"{seed}_test_data.pkl\", \"rb\") as f:\n",
    "#     test_data = pickle.load(f)\n",
    "\n",
    "# permutations = list(opc.results.keys())\n",
    "# num_optimal = test_data[\"num_optimal\"]\n",
    "# num_suboptimal = test_data[\"num_suboptimal\"]\n",
    "# num_failed = test_data[\"num_failed\"]\n",
    "# total_excess_steps = test_data[\"total_excess_steps\"]\n",
    "# num_steps_list = test_data[\"num_steps_list\"]\n",
    "# skipped = test_data[\"skipped\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(opc.results.keys()) - set(list_most_optimal) - set(list_optimal) - set(list_suboptimal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tested configurations: 2250000\n",
      "Optimal: 1880478\n",
      "Suboptimal: 369522\n",
      "Failed to solve: 0\n",
      "Illegal config skipped: 0\n",
      "Average excess steps: 0.2264311111111111\n",
      "Percentage of cases solved under 15 steps: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tested configurations: {len(permutations) - skipped}\")\n",
    "print(f\"Optimal: {num_optimal}\")\n",
    "print(f\"Suboptimal: {num_suboptimal}\")\n",
    "print(f\"Failed to solve: {num_failed}\")\n",
    "print(f\"Illegal config skipped: {skipped}\")\n",
    "print(f\"Average excess steps: {total_excess_steps / (len(permutations) - skipped)}\")\n",
    "print(f\"Percentage of cases solved under 15 steps: {(len(permutations) - skipped - len([x for x in num_steps_list if x > 15]) - num_failed) / (len(permutations) - skipped) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Seed      | Scenarios Solved in Less Than 15 Steps | Epochs (Episodes) Required to Train to This Level | Average Excess Path Length (Step Count) |\n",
    "|-----------|-----------------------------------------|---------------------------------------------------|------------------------------------------|\n",
    "| 34553312  | 100%                                    | 19999                                             | 0.23790311111111112                                 |\n",
    "| 32459525  | 100%                                    | 19999                                             | 0.1933408888888889                       |\n",
    "| 17287966  | 100%                                    | 19999                                             | 0.25179644444444443                      |\n",
    "| 3210744   | 100%                                    | 19999                                             | 0.2264311111111111                       |\n",
    "| 28189744  | 100%                                    | 19999                                             | 0.225252                       |\n",
    "| 20067541  | 100%                                    | 19999                                             | 0.23992222222222223                      |\n",
    "| 32773548  | 100%                                    | 19999                                             | 0.25206                       |\n",
    "| 19616403  | 100%                                    | 19999                                             | 0.234536                       |\n",
    "| 33254042  | 100%                                    | 19999                                             | 0.21858577777777777                       |\n",
    "| 33725772  | 100%                                    | 19999                                             | 0.25937955555555553                                  |\n",
    "\n",
    "**Note:** These results were obtained with the seeds seen on the leftmost column. The seeds can be applied at the beginning of this notebook to fully and completely reproduce these results.\n",
    "\n",
    "**Note 2:** The seeds were generated randomly using random.org to separate the seeding from the codebase.\n",
    "\n",
    "**Note 3:** The Average Excess Path Length (Step Count) was calculated with the optimal path length calculated **with NO WAIT ACTION** taken into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of interactive visualisations that I included in this project. Each type of visualisation can be accessed by pressing the respective button.\n",
    "\n",
    "### Viewing Training Episode (**View Training Episode button**)\n",
    "1. Enter the episode you want to view\n",
    "2. Click Submit\n",
    "\n",
    "or alternatively\n",
    "\n",
    "1. Click *Randomise Episode* to view a random episode\n",
    "\n",
    "### Testing the agents with a specified starting configuration (**Test with Specified Position button**)\n",
    "1. Enter the starting configuration for agents of type 1. The input can either be (x, y, x, y), ((x, y), (x, y)), or (x, y), (x, y)\n",
    "2. Enter the starting configuration for agents of type 2. The input can either be (x, y, x, y), ((x, y), (x, y)), or (x, y), (x, y)\n",
    "3. Enter the position for B. The input can either be (x, y) or x, y\n",
    "3. Click Submit\n",
    "\n",
    "or alternatively\n",
    "\n",
    "1. Click *Random Position* to test with random starting positions.\n",
    "\n",
    "**NOTE**: The numbering of the grid starts from 0, with the top left corner being (0, 0) and the bottom right corner being (GRID_HEIGHT - 1, GRID_WIDTH - 1).\n",
    "\n",
    "You can now use the navigation buttons to go forward and backward in the episode. Please note that:\n",
    "1. Next Substep/Previous Substep will go 1 substep forward/backward. This will move the episode along agent by agent.\n",
    "2. Next Step/Previous Step will go 1 step forward/backward. This will move the episode along step by step.\n",
    "+ The Next Step button will always iterate to the last recorded substep of a given step. For example, if you are currently at Step 2 (2/4), you will be moved to Step 2(4/4) then subsequently to Step 3(4/4). \n",
    "+ The Previous Step button will always iterate to the last recorded substep of a given step. For example, if you are currently at Step 3 (2/4), you will be moved to Step 2(4/4) then subsequently to Step 1(4/4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Button, TextBox\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to wipe the plot clean and reconstruct the grid\n",
    "def reset_plot():\n",
    "    global fig, ax\n",
    "\n",
    "    ax.clear()\n",
    "\n",
    "    plt.subplots_adjust(bottom = 0.35)\n",
    "\n",
    "    # Move the x axis to the top of the plot and invert the y axis\n",
    "    ax.xaxis.tick_top()\n",
    "\n",
    "    if not ax.yaxis_inverted():\n",
    "        ax.invert_yaxis()\n",
    "    \n",
    "\n",
    "    # Set the ticks of the x and y axis to align with the grid\n",
    "    ax.set_xticks(np.arange(0, GRID_WIDTH + 1, 1))\n",
    "    ax.set_yticks(np.arange(0, GRID_HEIGHT + 1, 1))\n",
    "\n",
    "    # Make the ticks and the numbers on the axis invisible\n",
    "    ax.tick_params(axis='both', which='both', length=0)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    # Display the grid\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Make the plot square\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to display the state on the grid\n",
    "def display_state_on_grid(state: tuple):\n",
    "    global fig, ax, agent_mapping\n",
    "\n",
    "    # Clear the plot\n",
    "    reset_plot()\n",
    "\n",
    "    # print(state)\n",
    "    tmp_agent_1a = Agent1()\n",
    "    tmp_agent_1b = Agent1()\n",
    "    tmp_agent_2a = Agent2()\n",
    "    tmp_agent_2b = Agent2()\n",
    "    \n",
    "    tmp_agent_1a.reset_with_params(*state[0][0])\n",
    "    tmp_agent_1b.reset_with_params(*state[0][1])\n",
    "    tmp_agent_2a.reset_with_params(*state[0][2])\n",
    "    tmp_agent_2b.reset_with_params(*state[0][3])\n",
    "\n",
    "    b_pos = state[0][-1]\n",
    "\n",
    "    # Colours (in order 1a, 1b, 2a, 2b)\n",
    "    colours = ['red', 'red', 'green', 'green']\n",
    "\n",
    "    if tmp_agent_1a.has_secret:\n",
    "        colours[0] = 'deeppink'\n",
    "    if tmp_agent_1b.has_secret:\n",
    "        colours[1] = 'deeppink'\n",
    "    if tmp_agent_2a.has_secret:\n",
    "        colours[2] = 'mediumseagreen'\n",
    "    if tmp_agent_2b.has_secret:\n",
    "        colours[3] = 'mediumseagreen'\n",
    "\n",
    "    # Display agent 1s\n",
    "    # ax.add_patch(plt.Rectangle((state.a_x, state.a_y), 1, 1, color='red'))\n",
    "    ax.add_patch(plt.Rectangle((tmp_agent_1a.agent_pos[0], tmp_agent_1a.agent_pos[1]), 1, 1, color=colours[0]))\n",
    "    ax.add_patch(plt.Rectangle((tmp_agent_1b.agent_pos[0], tmp_agent_1b.agent_pos[1]), 1, 1, color=colours[1]))\n",
    "\n",
    "    # Display agent 2s\n",
    "    # ax.add_patch(plt.Rectangle((state.b_x, state.b_y), 1, 1, color='green'))\n",
    "    ax.add_patch(plt.Rectangle((tmp_agent_2a.agent_pos[0], tmp_agent_2a.agent_pos[1]), 1, 1, color=colours[2]))\n",
    "    ax.add_patch(plt.Rectangle((tmp_agent_2b.agent_pos[0], tmp_agent_2b.agent_pos[1]), 1, 1, color=colours[3]))\n",
    "\n",
    "    # Display the delivery point\n",
    "    # ax.add_patch(plt.Rectangle((state.agent_x, state.agent_y), 1, 1, color='blue'))\n",
    "    ax.add_patch(plt.Rectangle((b_pos[0], b_pos[1]), 1, 1, color='blue'))\n",
    "\n",
    "    # Check if agent 1s and agent 2s are at the same position\n",
    "    for agent in [tmp_agent_1a, tmp_agent_1b]:\n",
    "        if agent.agent_pos in [tmp_agent_2a.agent_pos, tmp_agent_2b.agent_pos]:\n",
    "            ax.add_patch(plt.Rectangle((agent.agent_pos[0], agent.agent_pos[1]), 1, 1, color='darksalmon'))\n",
    "    \n",
    "    for agent in [tmp_agent_2a, tmp_agent_2b]:\n",
    "        if agent.agent_pos in [tmp_agent_1a.agent_pos, tmp_agent_1b.agent_pos]:\n",
    "            ax.add_patch(plt.Rectangle((agent.agent_pos[0], agent.agent_pos[1]), 1, 1, color='darksalmon'))\n",
    "    \n",
    "    # If any agent is at the same position as B\n",
    "    if b_pos in [tmp_agent_1a.agent_pos, tmp_agent_1b.agent_pos, tmp_agent_2a.agent_pos, tmp_agent_2b.agent_pos]:\n",
    "        ax.add_patch(plt.Rectangle((b_pos[0], b_pos[1]), 1, 1, color='darkturquoise'))\n",
    "\n",
    "    if (b_pos == tmp_agent_1a.agent_pos and tmp_agent_1a.has_secret) or (b_pos == tmp_agent_1b.agent_pos and tmp_agent_1b.has_secret) or (b_pos == tmp_agent_2a.agent_pos and tmp_agent_2a.has_secret) or (b_pos == tmp_agent_2b.agent_pos and tmp_agent_2b.has_secret):\n",
    "        ax.add_patch(plt.Rectangle((b_pos[0], b_pos[1]), 1, 1, color='dodgerblue'))\n",
    "\n",
    "    # Get grid size\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "\n",
    "    # Display legend\n",
    "    ax.text(-3 * x_max / 5, 2.1 * y_min / 5, \"Legend:\", fontsize=10)\n",
    "    ax.text(-3 * x_max / 5, 2.5 * y_min / 5, \"Agent 1s\", fontsize=10, color='red')\n",
    "    ax.text(-3 * x_max / 5, 2.9 * y_min / 5, \"Agent 1s /w secret\", fontsize=10, color='deeppink')\n",
    "    ax.text(-3 * x_max / 5, 3.3 * y_min / 5, \"Agent 2s\", fontsize=10, color='green')\n",
    "    ax.text(-3 * x_max / 5, 3.7 * y_min / 5, \"Agent 2s /w secret\", fontsize=10, color='mediumseagreen')\n",
    "    ax.text(-3 * x_max / 5, 4.1 * y_min / 5, \"Agent 1&2 overlap\", fontsize=10, color='darksalmon')\n",
    "    ax.text(-3 * x_max / 5, 4.5 * y_min / 5, \"B (Delivery point)\", fontsize=10, color='blue')\n",
    "    ax.text(-3 * x_max / 5, 4.9 * y_min / 5, \"Agent at B\", fontsize=10, color='darkturquoise')\n",
    "    ax.text(-3 * x_max / 5, 5.3 * y_min / 5, \"Agent at B /w secret\", fontsize=10, color='dodgerblue')\n",
    "\n",
    "    # Display agent 1s current position in text\n",
    "    agent_pos_text_y_pos = [2, 2.5, 3, 3.5]\n",
    "\n",
    "    ax.text(5.5 * x_max / 5, 1.5 * y_min / 5, f\"Agents' status\\n(In order):\", fontsize=10)\n",
    "    ax.text(5.5 * x_max / 5, agent_pos_text_y_pos[agent_mapping[\"1a\"]] * y_min / 5, f\"Agent 1a: {tmp_agent_1a.agent_pos}\", fontsize=10, color=colours[0])\n",
    "    ax.text(5.5 * x_max / 5, agent_pos_text_y_pos[agent_mapping[\"1b\"]] * y_min / 5, f\"Agent 1b: {tmp_agent_1b.agent_pos}\", fontsize=10, color=colours[1])\n",
    "    ax.text(5.5 * x_max / 5, agent_pos_text_y_pos[agent_mapping[\"2a\"]] * y_min / 5, f\"Agent 2a: {tmp_agent_2a.agent_pos}\", fontsize=10, color=colours[2])\n",
    "    ax.text(5.5 * x_max / 5, agent_pos_text_y_pos[agent_mapping[\"2b\"]] * y_min / 5, f\"Agent 2b: {tmp_agent_2b.agent_pos}\", fontsize=10, color=colours[3])\n",
    "\n",
    "    # plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to display the path the agent will take to reach the terminal state from its current position.\n",
    "def show_full_path():\n",
    "    global fig, ax, s, vis_states, state_index\n",
    "\n",
    "    for i, state in enumerate(vis_states): \n",
    "        # Skip states that have already been visited\n",
    "        if i <= state_index:\n",
    "            continue\n",
    "\n",
    "        ax.add_patch(plt.Rectangle((state.agent_x, state.agent_y), 1, 1, color=cm.viridis((state_index + i) / (len(vis_states) + state_index))))\n",
    "        \n",
    "\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to move the agent to the next state\n",
    "def next_substep(event):\n",
    "    global state_index, vis_states, text_current_move_count, button_show_path\n",
    "    state_index += 1\n",
    "\n",
    "    reset_plot()\n",
    "\n",
    "    if state_index >= len(vis_states):\n",
    "        state_index -= 1\n",
    "\n",
    "    # Update relevant labels\n",
    "    if state_index == 0:\n",
    "        text_current_move_count.set_text(f\"Current step count: 0 (0/0)\")\n",
    "    else:\n",
    "        text_current_move_count.set_text(f\"Current step count: {(state_index - 1) // 4 + 1} ({(state_index - 1) % 4 + 1}/4)\")\n",
    "    # button_show_path.label.set_text(\"Show Path\")\n",
    "\n",
    "    display_state_on_grid(vis_states[state_index])\n",
    "\n",
    "# Function used to move the agent to the previous state\n",
    "def previous_substep(event):\n",
    "    global state_index, vis_states, text_current_move_count, button_show_path\n",
    "    state_index -= 1\n",
    "\n",
    "    reset_plot()\n",
    "\n",
    "    if state_index < 0:\n",
    "        state_index = 0\n",
    "    \n",
    "    # Update relevant labels\n",
    "    if state_index == 0:\n",
    "        text_current_move_count.set_text(f\"Current step count: 0 (0/0)\")\n",
    "    else:\n",
    "        text_current_move_count.set_text(f\"Current step count: {(state_index - 1) // 4 + 1} ({(state_index - 1) % 4 + 1}/4)\")\n",
    "    # button_show_path.label.set_text(\"Show Path\")\n",
    "\n",
    "    display_state_on_grid(vis_states[state_index])\n",
    "\n",
    "def next_step(event):\n",
    "    global state_index, vis_states, text_current_move_count, button_show_path\n",
    "    state_index += 4\n",
    "\n",
    "    if state_index >= len(vis_states):\n",
    "        state_index = len(vis_states) - 1\n",
    "    else:\n",
    "        if state_index % 4 != 0:\n",
    "            state_index -= state_index % 4\n",
    "\n",
    "    reset_plot()\n",
    "\n",
    "    if state_index >= len(vis_states):\n",
    "        state_index -= 1\n",
    "\n",
    "    # Update relevant labels\n",
    "    if state_index == 0:\n",
    "        text_current_move_count.set_text(f\"Current step count: 0 (0/0)\")\n",
    "    else:\n",
    "        text_current_move_count.set_text(f\"Current step count: {(state_index - 1) // 4 + 1} ({(state_index - 1) % 4 + 1}/4)\")\n",
    "    # button_show_path.label.set_text(\"Show Path\")\n",
    "\n",
    "    display_state_on_grid(vis_states[state_index])\n",
    "\n",
    "# Function used to move the agent to the previous state\n",
    "def previous_step(event):\n",
    "    global state_index, vis_states, text_current_move_count, button_show_path\n",
    "    state_index -= 4\n",
    "\n",
    "    if state_index < 0:\n",
    "        state_index = 0\n",
    "    else:\n",
    "        if state_index % 4 != 0:\n",
    "            state_index += (4 - state_index % 4)\n",
    "\n",
    "    reset_plot()\n",
    "\n",
    "    if state_index < 0:\n",
    "        state_index = 0\n",
    "    \n",
    "    # Update relevant labels\n",
    "    if state_index == 0:\n",
    "        text_current_move_count.set_text(f\"Current step count: 0 (0/0)\")\n",
    "    else:\n",
    "        text_current_move_count.set_text(f\"Current step count: {(state_index - 1) // 4 + 1} ({(state_index - 1) % 4 + 1}/4)\")\n",
    "    # button_show_path.label.set_text(\"Show Path\")\n",
    "\n",
    "    display_state_on_grid(vis_states[state_index])\n",
    "\n",
    "# Function used to toggle the display of the full path\n",
    "def full_path_toggle(event):\n",
    "    global vis_states, state_index, button_show_path\n",
    "\n",
    "    # Rename triggering button\n",
    "    if button_show_path.label.get_text() == \"Show Path\":\n",
    "        button_show_path.label.set_text(\"Hide Path\")\n",
    "        show_full_path()\n",
    "    else:\n",
    "        button_show_path.label.set_text(\"Show Path\")\n",
    "        reset_plot()\n",
    "        display_state_on_grid(vis_states[state_index])\n",
    "\n",
    "    plt.draw()\n",
    "\n",
    "# Function used to process the text input for the agent's position and A's position\n",
    "def process_text(text):\n",
    "    s = text\n",
    "    s = s.replace(\" \", \"\")\n",
    "    s = s.replace(\"(\", \"\")\n",
    "    s = s.replace(\")\", \"\")\n",
    "\n",
    "    values = s.split(\",\")\n",
    "\n",
    "    if len(values) == 2:\n",
    "        return int(values[0]), int(values[1])\n",
    "    elif len(values) == 4:\n",
    "        return int(values[0]), int(values[1]), int(values[2]), int(values[3])\n",
    "    \n",
    "# Function used to set the elements position on the grid with specified positions\n",
    "def get_state(event):\n",
    "    global text_agent_2s_pos, text_agent_1s_pos, text_b_pos, state_index, text_optimal, text_current_move_count, text_total_move, vis_states, agent_mapping\n",
    "    agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y = process_text(text_agent_1s_pos.text)\n",
    "    agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y = process_text(text_agent_2s_pos.text)\n",
    "    b_x, b_y = process_text(text_b_pos.text)\n",
    "    \n",
    "    # Use the agent to go find the optimal path\n",
    "    world.reset_with_params(agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y)\n",
    "    \n",
    "    if world.agent_to_move_first == 1:\n",
    "        agent_mapping = {\n",
    "            \"1a\": 0,\n",
    "            \"1b\": 1,\n",
    "            \"2a\": 2,\n",
    "            \"2b\": 3\n",
    "        }\n",
    "        agents = [world.agent_1a, world.agent_1b, world.agent_2a, world.agent_2b]\n",
    "    else:\n",
    "        agent_mapping = {\n",
    "            \"2a\": 0,\n",
    "            \"2b\": 1,\n",
    "            \"1a\": 2,\n",
    "            \"1b\": 3\n",
    "        }\n",
    "        agents = [world.agent_2a, world.agent_2b, world.agent_1a, world.agent_1b]\n",
    "    \n",
    "    state_index = 0\n",
    "    optimal_move_count = opc.results[world.initial_config][0][3]\n",
    "    \n",
    "    text_optimal.set_text(f\"Optimal step: {optimal_move_count}\")\n",
    "    text_current_move_count.set_text(f\"Current step count: 0 (0/0)\")\n",
    "\n",
    "    vis_states = [(world.state.configuration(), None, None, None, None)]\n",
    "\n",
    "    current_step = 0\n",
    "    while not world.state.isTerminal() and current_step < GRID_HEIGHT * GRID_WIDTH * 2:\n",
    "        for i, agent in enumerate(agents):\n",
    "            # agent.opposite_pos = world.closest_opposite_agent(agent).agent_pos\n",
    "            action = agent.bestAction()\n",
    "            world.applyAction(agent, action)\n",
    "\n",
    "            vis_states.append((world.state.configuration(), agent.configuration(), action.value, current_step, i))\n",
    "\n",
    "            if world.state.isTerminal():\n",
    "                break\n",
    "        \n",
    "        current_step += 1\n",
    "\n",
    "    text_total_move.set_text(f\"Total step count: {current_step}\")\n",
    "\n",
    "    display_state_on_grid(vis_states[state_index])\n",
    "\n",
    "def get_state_random(event):\n",
    "    global text_agent_2s_pos, text_agent_1s_pos, text_b_pos, state_index, text_optimal, text_current_move_count, text_total_move, vis_states, agent_mapping\n",
    "    \n",
    "    agent_1a_x = np.random.randint(0, GRID_WIDTH)\n",
    "    agent_1a_y = np.random.randint(0, GRID_HEIGHT)\n",
    "    agent_1b_x = np.random.randint(0, GRID_WIDTH)\n",
    "    agent_1b_y = np.random.randint(0, GRID_HEIGHT)\n",
    "    agent_2a_x = np.random.randint(0, GRID_WIDTH)\n",
    "    agent_2a_y = np.random.randint(0, GRID_HEIGHT)\n",
    "    agent_2b_x = np.random.randint(0, GRID_WIDTH)\n",
    "    agent_2b_y = np.random.randint(0, GRID_HEIGHT)\n",
    "\n",
    "    while True:\n",
    "        b_x = np.random.randint(0, GRID_WIDTH)\n",
    "        b_y = np.random.randint(0, GRID_HEIGHT)\n",
    "\n",
    "        if (agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y) not in list_suboptimal:\n",
    "            if (b_x, b_y) not in [(agent_1a_x, agent_1a_y), (agent_1b_x, agent_1b_y), (agent_2a_x, agent_2a_y), (agent_2b_x, agent_2b_y)]:\n",
    "                break\n",
    "\n",
    "    \n",
    "    text_agent_1s_pos.set_val(f\"({agent_1a_x}, {agent_1a_y}, {agent_1b_x}, {agent_1b_y})\")\n",
    "    text_agent_2s_pos.set_val(f\"({agent_2a_x}, {agent_2a_y}, {agent_2b_x}, {agent_2b_y})\")\n",
    "    text_b_pos.set_val(f\"({b_x}, {b_y})\")\n",
    "\n",
    "    # Use the agent to go find the optimal path\n",
    "    world.reset_with_params(agent_1a_x, agent_1a_y, agent_1b_x, agent_1b_y, agent_2a_x, agent_2a_y, agent_2b_x, agent_2b_y, b_x, b_y)\n",
    "    \n",
    "    if world.agent_to_move_first == 1:\n",
    "        agent_mapping = {\n",
    "            \"1a\": 0,\n",
    "            \"1b\": 1,\n",
    "            \"2a\": 2,\n",
    "            \"2b\": 3\n",
    "        }\n",
    "        agents = [world.agent_1a, world.agent_1b, world.agent_2a, world.agent_2b]\n",
    "    else:\n",
    "        agent_mapping = {\n",
    "            \"2a\": 0,\n",
    "            \"2b\": 1,\n",
    "            \"1a\": 2,\n",
    "            \"1b\": 3\n",
    "        }\n",
    "        agents = [world.agent_2a, world.agent_2b, world.agent_1a, world.agent_1b]\n",
    "    \n",
    "    state_index = 0\n",
    "    optimal_move_count = opc.results[world.initial_config][0][3]\n",
    "    \n",
    "    text_optimal.set_text(f\"Optimal step: {optimal_move_count}\")\n",
    "    text_current_move_count.set_text(f\"Current step count: 0 (0/0)\")\n",
    "\n",
    "    vis_states = [(world.state.configuration(), None, None, None, None)]\n",
    "\n",
    "    current_step = 0\n",
    "    while not world.state.isTerminal() and current_step < GRID_HEIGHT * GRID_WIDTH * 2:\n",
    "        for i, agent in enumerate(agents):\n",
    "            # agent.opposite_pos = world.closest_opposite_agent(agent).agent_pos\n",
    "            action = agent.bestAction()\n",
    "            world.applyAction(agent, action)\n",
    "\n",
    "            vis_states.append((world.state.configuration(), agent.configuration(), action.value, current_step, i))\n",
    "\n",
    "            if world.state.isTerminal():\n",
    "                break\n",
    "        \n",
    "        current_step += 1\n",
    "\n",
    "    text_total_move.set_text(f\"Total step count: {current_step}\")\n",
    "\n",
    "    display_state_on_grid(vis_states[state_index])\n",
    "\n",
    "# Function used to set the episode to display the agent's training path\n",
    "def set_episode(event):\n",
    "    global state_index, text_optimal, text_current_move_count, text_total_move, vis_states, episode_history, text_episode_input, agent_mapping\n",
    "    episode = int(text_episode_input.text)\n",
    "\n",
    "    s = episode_history[episode]\n",
    "    state_index = 0\n",
    "    optimal_move_count = opc.results[s[\"initial_config\"]][0][3]\n",
    "    total_steps = s[\"history\"][-1][3] + 1\n",
    "\n",
    "    world.reset_with_params(*s[\"initial_config\"])\n",
    "\n",
    "    if world.agent_to_move_first == 1:\n",
    "        agent_mapping = {\n",
    "            \"1a\": 0,\n",
    "            \"1b\": 1,\n",
    "            \"2a\": 2,\n",
    "            \"2b\": 3\n",
    "        }\n",
    "    else:\n",
    "        agent_mapping = {\n",
    "            \"2a\": 0,\n",
    "            \"2b\": 1,\n",
    "            \"1a\": 2,\n",
    "            \"1b\": 3\n",
    "        }\n",
    "\n",
    "    text_optimal.set_text(f\"Optimal step count: {optimal_move_count}\")\n",
    "    text_current_move_count.set_text(f\"Current step count: 0 (0/0)\")\n",
    "    # text_current_sub_step.set_text(f\"Current sub-step count: {s['history'][state_index][4]}\")\n",
    "    text_total_move.set_text(f\"Total step count: {total_steps}\")\n",
    "    vis_states = [(world.state.configuration(), None, None, None, None)]\n",
    "    vis_states += s[\"history\"]\n",
    "    display_state_on_grid(vis_states[state_index])\n",
    "\n",
    "\n",
    "\n",
    "def set_random_episode(event):\n",
    "    global state_index, text_optimal, text_current_move_count, text_total_move, vis_states, episode_history, text_episode_input, agent_mapping\n",
    "    episode = np.random.randint(0, len(episode_history))\n",
    "    text_episode_input.set_val(str(episode))\n",
    "\n",
    "    s = episode_history[episode]\n",
    "    state_index = 0\n",
    "    optimal_move_count = opc.results[s[\"initial_config\"]][0][3]\n",
    "    total_steps = s[\"history\"][-1][3] + 1\n",
    "\n",
    "    world.reset_with_params(*s[\"initial_config\"])\n",
    "\n",
    "    if world.agent_to_move_first == 1:\n",
    "        agent_mapping = {\n",
    "            \"1a\": 0,\n",
    "            \"1b\": 1,\n",
    "            \"2a\": 2,\n",
    "            \"2b\": 3\n",
    "        }\n",
    "    else:\n",
    "        agent_mapping = {\n",
    "            \"2a\": 0,\n",
    "            \"2b\": 1,\n",
    "            \"1a\": 2,\n",
    "            \"1b\": 3\n",
    "        }\n",
    "\n",
    "    text_optimal.set_text(f\"Optimal step count: {optimal_move_count}\")\n",
    "    text_current_move_count.set_text(f\"Current step count: 0 (0/0)\")\n",
    "    # text_current_sub_step.set_text(f\"Current sub-step count: {s['history'][state_index][4]}\")\n",
    "    text_total_move.set_text(f\"Total step count: {total_steps}\")\n",
    "    vis_states = [(world.state.configuration(), None, None, None, None)]\n",
    "    vis_states += s[\"history\"]\n",
    "    display_state_on_grid(vis_states[state_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_training_episode(event):\n",
    "    # All variables will be global\n",
    "    global fig, ax, ax_next_substep, ax_previous_substep, ax_next_step, ax_previous_step, ax_show_path, ax_submit_pos, ax_view_training_episode, ax_test_with_random_pos, button_next_substep, button_previous_substep, button_next_step, button_previous_step, button_show_path, button_submit_pos, button_randomise_pos, button_view_training_episode, button_test_with_random_pos, ax_episode_input, text_episode_input, text_optimal, text_current_move_count, text_total_move, vis_states, state_index\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Reset\n",
    "    reset_plot()\n",
    "    vis_states = None\n",
    "    state_index = None\n",
    "\n",
    "    # Creating buttons\n",
    "    # ax_next = plt.axes([0.81, 0.05, 0.1, 0.075])\n",
    "    # ax_previous = plt.axes([0.7, 0.05, 0.1, 0.075])\n",
    "    # ax_show_path = plt.axes([0.7, 0.15, 0.2, 0.075])\n",
    "    ax_next_substep = plt.axes([0.81, 0.15, 0.1, 0.075])\n",
    "    ax_previous_substep = plt.axes([0.7, 0.15, 0.1, 0.075])\n",
    "    ax_next_step = plt.axes([0.81, 0.05, 0.1, 0.075])\n",
    "    ax_previous_step = plt.axes([0.7, 0.05, 0.1, 0.075])\n",
    "    ax_randomise_pos = plt.axes([0.41, 0.05, 0.2, 0.075])\n",
    "    ax_submit_pos = plt.axes([0.2, 0.05, 0.2, 0.075])\n",
    "\n",
    "    ax_view_training_episode = plt.axes([0.05, 0.7, 0.2, 0.075])\n",
    "    ax_test_with_random_pos = plt.axes([0.05, 0.8, 0.2, 0.075])\n",
    "\n",
    "    # button_show_path = Button(ax_show_path, 'Show Path')\n",
    "    button_next_substep = Button(ax_next_substep, 'Next\\nSubstep')\n",
    "    button_previous_substep = Button(ax_previous_substep, 'Previous\\nSubstep')\n",
    "    button_next_step = Button(ax_next_step, 'Next\\nFull Step')\n",
    "    button_previous_step = Button(ax_previous_step, 'Previous\\nFull Step')\n",
    "    button_submit_pos = Button(ax_submit_pos, 'Submit')\n",
    "    button_randomise_pos = Button(ax_randomise_pos, 'Randomise\\nEpisode')\n",
    "    button_view_training_episode = Button(ax_view_training_episode, 'View Training\\nEpisode')\n",
    "    button_test_with_random_pos = Button(ax_test_with_random_pos, 'Test with\\nSpecified Position')\n",
    "\n",
    "    # Create episode input text boxes\n",
    "    ax_episode_input = plt.axes([0.4, 0.15, 0.1, 0.075])\n",
    "    text_episode_input = TextBox(ax_episode_input, f'Input episode 0 to {len(episode_history) - 1} ', initial=\"0\")\n",
    "\n",
    "    # Create text labels\n",
    "    text_optimal = plt.text(-3, 10.5, f\"\")\n",
    "    text_current_move_count = plt.text(0, 10.5, f\"\")\n",
    "    text_total_move = plt.text(3.5, 10.5, f\"\")\n",
    "\n",
    "    # Connect buttons to functions\n",
    "    button_next_substep.on_clicked(next_substep)\n",
    "    button_previous_substep.on_clicked(previous_substep)\n",
    "    # button_show_path.on_clicked(full_path_toggle)\n",
    "    button_next_step.on_clicked(next_step)\n",
    "    button_previous_step.on_clicked(previous_step)\n",
    "    button_submit_pos.on_clicked(set_episode)\n",
    "    button_randomise_pos.on_clicked(set_random_episode)\n",
    "\n",
    "\n",
    "    button_view_training_episode.on_clicked(view_training_episode)\n",
    "    button_test_with_random_pos.on_clicked(test_with_random_pos)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_random_pos(event):\n",
    "    global fig, ax, ax_next, ax_previous, ax_show_path, ax_submit_pos, ax_view_training_episode, ax_test_with_random_pos, button_next_substep, button_previous_substep, button_next_step, button_previous_step, button_show_path, button_submit_pos, button_randomise_pos, button_view_training_episode, button_test_with_random_pos, text_optimal, text_current_move_count, text_total_move, ax_agent_1s_pos, text_agent_1s_pos, ax_agent_2s_pos, text_agent_2s_pos, text_b_pos, vis_states, state_index\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Reset\n",
    "    reset_plot()\n",
    "    vis_states = None\n",
    "    state_index = None\n",
    "\n",
    "    # Creating buttons\n",
    "    # ax_next = plt.axes([0.81, 0.05, 0.1, 0.075])\n",
    "    # ax_previous = plt.axes([0.7, 0.05, 0.1, 0.075])\n",
    "    # ax_show_path = plt.axes([0.7, 0.15, 0.2, 0.075])\n",
    "    ax_next_substep = plt.axes([0.81, 0.15, 0.1, 0.075])\n",
    "    ax_previous_substep = plt.axes([0.7, 0.15, 0.1, 0.075])\n",
    "    ax_next_step = plt.axes([0.81, 0.05, 0.1, 0.075])\n",
    "    ax_previous_step = plt.axes([0.7, 0.05, 0.1, 0.075])\n",
    "    ax_submit_pos = plt.axes([0.41, 0.15, 0.2, 0.075])\n",
    "    ax_randomise_pos = plt.axes([0.41, 0.05, 0.2, 0.075])\n",
    "\n",
    "    ax_view_training_episode = plt.axes([0.05, 0.7, 0.2, 0.075])\n",
    "    ax_test_with_random_pos = plt.axes([0.05, 0.8, 0.2, 0.075])\n",
    "\n",
    "    # button_show_path = Button(ax_show_path, 'Show Path')\n",
    "    button_next_substep = Button(ax_next_substep, 'Next\\nSubstep')\n",
    "    button_previous_substep = Button(ax_previous_substep, 'Previous\\nSubstep')\n",
    "    button_next_step = Button(ax_next_step, 'Next\\nFull Step')\n",
    "    button_previous_step = Button(ax_previous_step, 'Previous\\nFull Step')\n",
    "    button_submit_pos = Button(ax_submit_pos, 'Submit')\n",
    "    button_randomise_pos = Button(ax_randomise_pos, 'Randomise\\nPosition')\n",
    "    \n",
    "    button_view_training_episode = Button(ax_view_training_episode, 'View Training\\nEpisode')\n",
    "    button_test_with_random_pos = Button(ax_test_with_random_pos, 'Test with\\nSpecified Position')\n",
    "\n",
    "    # Create text labels\n",
    "    text_optimal = plt.text(0.2, 1.8, f\"\")\n",
    "    text_current_move_count = plt.text(1.7, 1.8, f\"\")\n",
    "    text_total_move = plt.text(3.4, 1.8, f\"\")\n",
    "\n",
    "    # Create text box to select a position\n",
    "    ax_agent_1s_pos = plt.axes([0.25, 0.21, 0.135, 0.075])\n",
    "    text_agent_1s_pos = TextBox(ax_agent_1s_pos, 'Agent 1s positions\\n(x, y, x, y) ', initial=\"0\")\n",
    "\n",
    "    ax_agent_2s_pos = plt.axes([0.25, 0.13, 0.135, 0.075])\n",
    "    text_agent_2s_pos = TextBox(ax_agent_2s_pos, 'Agent 2s positions\\n(x, y, x, y) ', initial=\"0\")\n",
    "\n",
    "    ax_b_pos = plt.axes([0.25, 0.05, 0.135, 0.075])\n",
    "    text_b_pos = TextBox(ax_b_pos, 'B position (x, y) ', initial=\"0\")\n",
    "\n",
    "    # Connect buttons to functions\n",
    "    button_next_substep.on_clicked(next_substep)\n",
    "    button_previous_substep.on_clicked(previous_substep)\n",
    "    # button_show_path.on_clicked(full_path_toggle)\n",
    "    button_next_step.on_clicked(next_step)\n",
    "    button_previous_step.on_clicked(previous_step)\n",
    "    button_submit_pos.on_clicked(get_state)\n",
    "    button_randomise_pos.on_clicked(get_state_random)\n",
    "    \n",
    "    button_view_training_episode.on_clicked(view_training_episode)\n",
    "    button_test_with_random_pos.on_clicked(test_with_random_pos)\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_training_episode(None)\n",
    "test_with_random_pos(None)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
